{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb706c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, csv\n",
    "from bs4 import BeautifulSoup\n",
    "from os import path\n",
    "\n",
    "import scripts.scraper as scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b1b485",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800d2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from url\n",
    "url1 = 'https://www.amazon.fr/EOS-2000D-Kit-18-55mm-Spiegelreflexkamera/product-reviews/B07FDG71K9/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'\n",
    "url2 = 'https://www.amazon.fr/kindle-maintenant-avec-un-eclairage-frontal-integre-noir/product-reviews/B07FQ4XCR1/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'\n",
    "url3 = 'https://www.amazon.fr/Braun-81710912-Series-7-70-S4200cs/product-reviews/B089GQTM3C/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'\n",
    "html_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09951819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from files\n",
    "url4 = 'html/Braun1.html'\n",
    "url5 = 'html/Braun2.html'\n",
    "url6 = 'html/Braun3.html'\n",
    "url7 = 'html/Braun4.html'\n",
    "\n",
    "html_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915de168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "10 reviews scraped\n",
      "Scraping page 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Amazon robot detection, try later",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50252/1571316222.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscraper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrap_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/personal-projects/amazon-review-score-prediction/scripts/scraper.py\u001b[0m in \u001b[0;36mscrap_pages\u001b[0;34m(url, start, end)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Scraping page {page}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{url}&pageNumber={page}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_reviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/personal-projects/amazon-review-score-prediction/scripts/scraper.py\u001b[0m in \u001b[0;36mget_soup\u001b[0;34m(url, html_file)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a-last'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a-last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Désolés, il faut que nous nous assurions que vous n'êtes pas un robot. Pour obtenir les meilleurs résultats, veuillez vous assurer que votre navigateur accepte les cookies.\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Amazon robot detection, try later'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unknown error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Amazon robot detection, try later"
     ]
    }
   ],
   "source": [
    "url = url3\n",
    "\n",
    "start = 1\n",
    "end = 5\n",
    "\n",
    "reviews = scraper.scrap_pages(url, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98d044ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 reviews added to existing file\n"
     ]
    }
   ],
   "source": [
    "if reviews:\n",
    "    scraper.reviews_to_csv(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b633176",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae0e5e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "10 reviews scraped\n",
      "Scraping page 2\n",
      "4 reviews scraped\n",
      "Scraping page 3\n",
      "0 reviews scraped\n",
      "Scraping page 4\n",
      "0 reviews scraped\n",
      "Scraping page 5\n",
      "0 reviews scraped\n",
      "All pages scraped, 14 reviews\n",
      "14 reviews added to existing file\n"
     ]
    }
   ],
   "source": [
    "url = url3\n",
    "\n",
    "start = 1\n",
    "end = 5\n",
    "\n",
    "def scrap_pages(url, start, end):\n",
    "    \n",
    "    reviews = []\n",
    "\n",
    "    for page in range(start, end+1):\n",
    "\n",
    "        print(f'Scraping page {page}')\n",
    "\n",
    "        soup = scraper.get_soup(f'{url}&pageNumber={page}')\n",
    "\n",
    "        for review in scraper.get_reviews(soup):\n",
    "            reviews.append(review)\n",
    "\n",
    "        if soup.find('li', class_='a-disabled a-last'):\n",
    "            print('Last page, stop')\n",
    "            break\n",
    "\n",
    "        if page == end:\n",
    "            print(f'All pages scraped, {len(reviews)} reviews')\n",
    "\n",
    "    if reviews:\n",
    "        scraper.reviews_to_csv(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f706fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup made from url\n",
      "Scraping reviews...\n",
      "10 reviews scraped\n",
      "Rows added to existing file\n"
     ]
    }
   ],
   "source": [
    "urls = [url4, url5, url6, url7]\n",
    "urls = [url2]\n",
    "\n",
    "for url in urls:\n",
    "    soup = scraper.get_soup(url, html_file=html_file)\n",
    "    reviews = scraper.get_reviews(soup)\n",
    "    if reviews:\n",
    "        scraper.reviews_to_csv(reviews)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
