{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb706c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, csv\n",
    "from bs4 import BeautifulSoup\n",
    "from os import path\n",
    "\n",
    "import scripts.scraper as scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b1b485",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800d2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from url\n",
    "url1 = 'https://www.amazon.fr/EOS-2000D-Kit-18-55mm-Spiegelreflexkamera/product-reviews/B07FDG71K9/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'\n",
    "url2 = 'https://www.amazon.fr/kindle-maintenant-avec-un-eclairage-frontal-integre-noir/product-reviews/B07FQ4XCR1/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'\n",
    "url3 = 'https://www.amazon.fr/Braun-81710912-Series-7-70-S4200cs/product-reviews/B089GQTM3C/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews'\n",
    "html_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09951819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from files\n",
    "url4 = 'html/Braun1.html'\n",
    "url5 = 'html/Braun2.html'\n",
    "url6 = 'html/Braun3.html'\n",
    "url7 = 'html/Braun4.html'\n",
    "\n",
    "html_file = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "915de168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "10 reviews scraped\n",
      "Scraping page 2\n",
      "4 reviews scraped\n",
      "Scraping page 3\n",
      "0 reviews scraped\n",
      "Scraping page 4\n",
      "0 reviews scraped\n",
      "Scraping page 5\n",
      "0 reviews scraped\n",
      "All pages scraped, 14 reviews\n"
     ]
    }
   ],
   "source": [
    "url = url3\n",
    "\n",
    "start = 1\n",
    "end = 5\n",
    "\n",
    "reviews = scraper.scrap_pages(url, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98d044ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 reviews added to existing file\n"
     ]
    }
   ],
   "source": [
    "if reviews:\n",
    "    scraper.reviews_to_csv(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b633176",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae0e5e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "10 reviews scraped\n",
      "Scraping page 2\n",
      "4 reviews scraped\n",
      "Scraping page 3\n",
      "0 reviews scraped\n",
      "Scraping page 4\n",
      "0 reviews scraped\n",
      "Scraping page 5\n",
      "0 reviews scraped\n",
      "All pages scraped, 14 reviews\n",
      "14 reviews added to existing file\n"
     ]
    }
   ],
   "source": [
    "url = url3\n",
    "\n",
    "start = 1\n",
    "end = 5\n",
    "\n",
    "def scrap_pages(url, start, end):\n",
    "    \n",
    "    reviews = []\n",
    "\n",
    "    for page in range(start, end+1):\n",
    "\n",
    "        print(f'Scraping page {page}')\n",
    "\n",
    "        soup = scraper.get_soup(f'{url}&pageNumber={page}')\n",
    "\n",
    "        for review in scraper.get_reviews(soup):\n",
    "            reviews.append(review)\n",
    "\n",
    "        if soup.find('li', class_='a-disabled a-last'):\n",
    "            print('Last page, stop')\n",
    "            break\n",
    "\n",
    "        if page == end:\n",
    "            print(f'All pages scraped, {len(reviews)} reviews')\n",
    "\n",
    "    if reviews:\n",
    "        scraper.reviews_to_csv(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f706fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soup made from url\n",
      "Scraping reviews...\n",
      "10 reviews scraped\n",
      "Rows added to existing file\n"
     ]
    }
   ],
   "source": [
    "urls = [url4, url5, url6, url7]\n",
    "urls = [url2]\n",
    "\n",
    "for url in urls:\n",
    "    soup = scraper.get_soup(url, html_file=html_file)\n",
    "    reviews = scraper.get_reviews(soup)\n",
    "    if reviews:\n",
    "        scraper.reviews_to_csv(reviews)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
